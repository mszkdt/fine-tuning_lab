{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mszkdt/fine-tuning_lab/blob/main/Piper_FineTuning_0528.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ２．学習済LLMのカスタマイズ：Fine Tuning（追加学習）\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xLMOnC5QIJv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![001_What_is_LLM](https://raw.githubusercontent.com/mszkdt/fine-tuning_lab/main/001_What_is_LLM.png)\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "![002_LLM_model_size](https://raw.githubusercontent.com/mszkdt/fine-tuning_lab/main/002_LLM_model_size.png)\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "![003_Fine-tuning.png](https://raw.githubusercontent.com/mszkdt/fine-tuning_lab/main/003_Fine-tuning.png)\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Js20gw-QJnR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 一般的なファインチューニング実施までの工程\n",
        "\n",
        "**1. 環境、データの準備**\n",
        "\n",
        "* ファイチューニング実行環境と、モデルの訓練に使用するデータを準備します。\n",
        "* これには、データの収集、クリーニング、前処理などが含まれます。\n",
        "\n",
        "**2. トークナイザーの準備**\n",
        "\n",
        "* 次に、テキストデータをモデルが理解できる形式に変換するためのトークナイザーを準備します。\n",
        "* トークナイザーは、テキストをトークン（単語や文字などの単位）に分割し、それぞれのトークンを一意の数値（トークンID）に変換します。\n",
        "\n",
        "**3. モデルの選択**\n",
        "\n",
        "* 事前訓練済みのモデルを選択します。\n",
        "* このモデルは、大量のテキストデータで事前に訓練されており、一般的な言語理解能力を持っています。\n",
        "\n",
        "**4. データのトークナイゼーション**\n",
        "\n",
        "* 準備したデータをトークナイザーでトークナイズし、モデルが理解できる形式に変換します。\n",
        "\n",
        "**5. ファインチューニングの設定、実施**\n",
        "\n",
        "* 最後に、ファインチューニングの設定を行います。\n",
        "* これには、訓練のパラメータ（例えば、学習率やエポック数など）の設定、訓練データの指定、モデルの保存先の指定などが含まれます。\n",
        "\n",
        "----\n",
        "\n"
      ],
      "metadata": {
        "id": "LjUuGDiGrPPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 今回のLab内容\n",
        "\n",
        "* Google Colabにて、 サイバーエージェントの OpenCalm を使い、iDRACのURIを学習させる\n",
        "* 学習前と学習後の実行結果を確認する"
      ],
      "metadata": {
        "id": "u1Ke1Cf3uoF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 参考：OpenCalm（おーぷんかーむ）とは\n",
        "\n",
        "サイバーエージェントが2023年5月11日に公開された日本語LLM\n",
        "\n",
        "*   商用利用が可能\n",
        "*   データソース：日本語のオープンデータ(Wikipedia, Common Crawl)\n",
        "*   パラメータ数： S 1.6億、 M 4億、 L 8.3億、 calm2 70億\n",
        "*   AI開発基盤にPowerEdge XE9680、 NVIDIA® H100 GPUを利用\n",
        "*   公開先： https://huggingface.co/cyberagent\n",
        "*   開発者： https://www.cyberagent.co.jp/way/list/detail/id=29497\n",
        "\n"
      ],
      "metadata": {
        "id": "NGGPfgz-IvN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 環境準備\n",
        "\n",
        "\n",
        "**実行前にFineTuning用データを、自身のGoogle Driveへダウンロードする**\n",
        "\n",
        "\"/content/drive/MyDrive/redfish_llm.csv\"\n",
        "\n",
        "\n",
        "\n",
        "**pipで以下ライブラリをインストール**\n",
        "\n",
        "* datasets\n",
        "  * 機械学習や自然言語処理のための大規模なデータセットを効率的にロード、前処理、探索するためのツール\n",
        "\n",
        "* accelerate\n",
        "  * 同じPyTorchのコードを任意の分散設定で実行できるようにし、大規模な訓練と推論をシンプルで効率的かつ適応可能にするツール\n",
        "\n",
        "* transformer\n",
        "  * トランスフォーマーモデル（BERT、GPT-2など）を簡単に利用できるようにするためのツール\n",
        "  * [torch]の部分は、このライブラリをPyTorchとともに使用するためのオプション"
      ],
      "metadata": {
        "id": "hxoCBF7YSs-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "jEgVqB_zHOzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### GoogleDriveマウント（学習データの一時保管）\n",
        "#\n",
        "# auth.authenticate_user()\n",
        "\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "h4KvAdLVKJtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習データのダウンロード\n",
        "\n",
        "!curl https://raw.githubusercontent.com/mszkdt/fine-tuning_lab/main/redfish_llm.csv > /content/drive/MyDrive/redfish_llm.csv"
      ],
      "metadata": {
        "id": "2gQxYyHhWv0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install transformers[torch]\n"
      ],
      "metadata": {
        "id": "O81jeh5KqXqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**インストールしたライブラリから、以下をインポート**\n",
        "\n",
        "* import pandas as pd\n",
        "  * pandasというライブラリをインポートする。pandasは、Pythonでデータ分析を行うための強力なライブラリで、データフレームという形式でデータを操作することができます。（pdは、pandasを参照するための省略形）\n",
        "\n",
        "* from datasets import Dataset, DatasetDict\n",
        "  * この行は、datasetsライブラリからDatasetとDatasetDictというクラスをインポートしています。これらのクラスは、Hugging Face社が提供するdatasetsライブラリの一部で、機械学習のデータセットを効率的に操作するためのものです。\n",
        "\n",
        "* import transformers\n",
        "  * transformersというライブラリ全体をインポートしています。transformersライブラリは、Hugging Face社が提供するもので、トランスフォーマーモデルを簡単に利用できるようにするためのものです。\n",
        "\n",
        "* import torch\n",
        "  * torchというライブラリをインポートしています。torchは、PyTorchとも呼ばれ、深層学習のためのオープンソースライブラリです。PyTorchは、tensor計算（NumPyのような）に強力なGPU加速を提供し、深層学習モデルの構築と訓練を支援します。\n"
      ],
      "metadata": {
        "id": "FDgEQL7yZqzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "import wandb\n",
        "wandb.init(mode=\"disabled\")\n"
      ],
      "metadata": {
        "id": "hV9ILFpDoxVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUが利用可能かどうかを確認\n",
        "#\n",
        "# 参考：https://note.com/npaka/n/n1aa6f8c973d0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "NpeP5_-k6hq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ファインチューニング準備\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "81UgPkNnVaB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルから pandas ライブラリを使用して、追加学習データ（CSV）を読み込む\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/redfish_llm.csv\")"
      ],
      "metadata": {
        "id": "IaC1Cqnfo1oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 追加学習データ（CSV）内容の表示\n",
        "df"
      ],
      "metadata": {
        "id": "lGAt6MFcqvfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ベースとなるLLMモデルの指定\n",
        "#\n",
        "# cyberagent/open-calm-small\n",
        "# cyberagent/open-calm-medium\n",
        "# cyberagent/open-calm-large\n",
        "# https://huggingface.co/cyberagent\n",
        "\n",
        "base_model = \"cyberagent/open-calm-medium\"\n",
        "\n",
        "# トークナイザーのロード（自然言語構造化された形式に変換し、モデルが理解できるようにする初期処理のゲートキーパー）\n",
        "# * transformersライブラリのAutoTokenizerクラスのfrom_pretrainedメソッドを使用して、事前学習済みのトークナイザーをロード\n",
        "# * from_pretrainedメソッドは、指定したモデル名（ここではbase_model）に対応するトークナイザーをロード\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# 事前学習済モデルのロード\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "# 追加学習データ用の前処理\n",
        "#\n",
        "# * トークナイザーを使用してテキストデータをモデルが理解できる形式に変換する\n",
        "#   テキストデータをトークン化（単語やサブワードに分割）し、それを数値のインデックスに変換する処理を行う。\n",
        "#\n",
        "# * \"pt\"はPyTorchのテンソル形式を意味し、トークナイザーはトークン化されたデータをPyTorchテンソルとして出力する。\n",
        "#   これにより、出力されたテンソルはそのままPyTorchを使用して構築されたモデルで学習に使用することができる。\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"input\"], return_tensors=\"pt\")\n",
        "    # return tokenizer(examples[\"input\"], padding=\"max_length\", max_length=8, truncation=True, return_tensors=\"pt\")\n",
        "    # return tokenizer(examples[\"input\"], padding=True, max_length=24, truncation=True, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "q70iQ8b7o7KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 追加学習データの加工\n",
        "#\n",
        "# * train_dataset = Dataset.from_pandas(df)\n",
        "#   pandasのデータフレームdfをHugging FaceのDatasetオブジェクトに変換する。\n",
        "#   Dataset.from_pandas(df)メソッドは、pandasのデータフレームを引数として受け取り、それをDatasetオブジェクトに変換する。\n",
        "#   この結果、データフレームの各行がDatasetの各エントリに対応します。\n",
        "\n",
        "# * data = DatasetDict({ \"train\": train_dataset, })\n",
        "#   変換したDatasetオブジェクトをDatasetDictオブジェクトに格納しています。\n",
        "#   DatasetDictは、複数のDatasetオブジェクトを一元管理するためのクラスで、各Datasetオブジェクトはキーを指定してアクセスできます。\n",
        "#   この例では、train_datasetを\"train\"のキーで格納しています。\n",
        "\n",
        "# * data = data.map(lambda samples: tokenizer(samples[\"output\"]), batched=True)\n",
        "#   data（DatasetDictオブジェクト）の各エントリに対してトークナイザーを適用しています。\n",
        "#\n",
        "#      lambda samples: tokenizer(samples[\"output\"])は、各エントリの\"output\"フィールドをトークナイザーに渡す無名関数。\n",
        "#      batched=Trueは、バッチ単位でトークナイゼーションを行うことを指定しています。\n",
        "#      これにより、一度に複数のエントリを処理することができ、効率的にトークナイゼーションを行うことができます。\n",
        "\n",
        "\n",
        "train_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "data = DatasetDict(\n",
        "    {\n",
        "        \"train\": train_dataset,\n",
        "    }\n",
        ")\n",
        "\n",
        "data = data.map(lambda samples: tokenizer(samples[\"output\"]), batched=True)"
      ],
      "metadata": {
        "id": "_wm_pvGSo-wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理後のdataから学習データセット（\"train\"キー）の2番目のエントリを取得している。\n",
        "# Pythonのインデックスは0から始まるため、[1]は2番目のエントリを指す。\n",
        "\n",
        "data[\"train\"][1]"
      ],
      "metadata": {
        "id": "nTQmr74cUNgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine Tuningの設定\n",
        "# *** pip XXXのエラーが消えない場合は、ランタイム >セッション再起動　で解決 ***\n",
        "\n",
        "# Hugging FaceのTransformersライブラリを使用して、モデルのFine Tuning（微調整）を設定するためのものです。\n",
        "\n",
        "# trainer = transformers.Trainer(...): ここでは、Trainerクラスのインスタンスを作成しています。Trainerクラスは、モデルの訓練や評価を行うためのクラス\n",
        "#    model=model: ここでは、微調整を行うモデルを指定\n",
        "#    train_dataset=data[\"train\"]: ここでは、訓練データセットを指定\n",
        "#    args=transformers.TrainingArguments(...): ここでは、訓練のパラメータを設定。TrainingArgumentsクラスは、訓練のパラメータを管理するためのクラス。\n",
        "#        logging_steps=1        : ログを出力するステップ数を指定しています。この例では、各ステップごとにログが出力されます。\n",
        "#        output_dir=\"./output\"  : モデルや訓練の結果を保存するディレクトリを指定しています。\n",
        "#        num_train_epochs = 5   : 訓練のエポック数を指定しています。この例では、5エポック訓練します。\n",
        "#                                 (機械学習の訓練プロセスにおける一つの単位を指す。具体的には、訓練データ全体がモデルに一度通過することを1エポック)\n",
        "#        warmup_steps = 5       : 学習率のウォームアップステップ数を指定しています。ウォームアップステップでは、学習率が徐々に上昇します。\n",
        "#        weight_decay = 0.1     : 重み減衰の値を指定しています。重み減衰は、過学習を防ぐため設定。\n",
        "#        save_steps = 10        : モデルを保存するステップ数を指定しています。この例では、10ステップごとにモデルが保存されます。\n",
        "#        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "#                                 ここでは、バッチ内のデータをまとめるためのDataCollatorを指定する。\n",
        "#                                 DataCollatorForLanguageModelingは、言語モデリングタスク（例えば、マスクされた言語モデリングや因果的言語モデリングなど）のためのDataCollator\n",
        "#                                 mlm=Falseは、マスクされた言語モデリングを行わないことを指定する。\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        # per_device_train_batch_size=4,\n",
        "        # gradient_accumulation_steps=4,\n",
        "        # warmup_steps=50,\n",
        "        # max_steps=500,\n",
        "        # warmup_steps=5,\n",
        "        # max_steps=200, # Epoch\n",
        "        # max_steps=10, # Epoch\n",
        "        # learning_rate=2e-4,\n",
        "        # fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"./output\",\n",
        "        num_train_epochs = 5,\n",
        "        warmup_steps = 10,\n",
        "        weight_decay = 0.1,\n",
        "        save_steps = 10,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")"
      ],
      "metadata": {
        "id": "inh1XZHMXUFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ファインチューニング実行、推論実行"
      ],
      "metadata": {
        "id": "nF6Sc76Aceh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推論（ファインチューニング前） ###"
      ],
      "metadata": {
        "id": "BpUz1jI70AHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# モデルに推論させるための入力テキストを設定\n",
        "input_text = \"GPUセンサーコレクションのURIは？\"\n",
        "# input_text = \"日本の有名な山は？\"\n",
        "# input_text = \"日本の有名な観光地は？\"\n",
        "\n",
        "# トークナイズ\n",
        "# 入力テキストをトークナイザーでトークナイズし、PyTorchのテンソルに変換して。そのテンソルをGPUまたはCPU（device）に送っている。\n",
        "input = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# モデルによる推論実行\n",
        "# with torch.no_grad(): ...:\n",
        "#     推論中に勾配の計算を行わないように指定しています。これにより、推論の速度が向上し、メモリ使用量が削減されます。\n",
        "#     モデルに入力テキストを与えて推論を行い、その結果を取得しています。max_new_tokens=24は、生成するトークンの最大数を指定しています。\n",
        "#     pad_token_id=tokenizer.pad_token_idは、生成するトークンが最大数に達した場合に追加するパディングトークンのIDを指定しています。\n",
        "\n",
        "# モデルによる推論実行\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**input, max_new_tokens=24, pad_token_id=tokenizer.pad_token_id, do_sample=True, temperature=0.7)\n",
        "\n",
        "# 出力テンソルをデコード（文字化）\n",
        "# skip_special_tokens=Trueは、特殊トークン（例えば、パディングトークンやセパレータートークンなど）をデコード結果から除外することを指定しています。\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# 結果を出力\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "4Wp3ZQK7YoM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ファインチューニングの実行"
      ],
      "metadata": {
        "id": "UJTCx4F90D1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "J7Oz51UOpC6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推論実行（ファインチューニング後） ###\n"
      ],
      "metadata": {
        "id": "rYGLhAW40I-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルに推論させるための入力テキストを設定\n",
        "input_text = \"GPUセンサーコレクションのURIは？\"\n",
        "\n",
        "# トークナイズ\n",
        "input = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# モデルによる推論実行\n",
        "# with torch.no_grad():\n",
        "#     output = model.generate(**input, max_new_tokens=24, pad_token_id=tokenizer.pad_token_id,)\n",
        "\n",
        "# モデルによる推論実行\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**input, max_new_tokens=24, pad_token_id=tokenizer.pad_token_id, do_sample=True, temperature=0.7)\n",
        "\n",
        "# 出力テンソルをデコード（文字化）\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# 結果を出力\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "V-QDyqtpDwvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルの内容を確認\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "D6Wh9kTbO5EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 参考：学習後のモデルを保存\n",
        "# trainer.save_model(\"./output/fine_tuned_model\")"
      ],
      "metadata": {
        "id": "0V0mQ18LpF61"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}